{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28741a72-d60f-4c99-bf91-026b68ec198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1db27b-e871-428d-abd8-556bfaacfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"D:/editor/DFD/process\"  # Parent folder containing \"real/\" and \"fake/\"\n",
    "FRAME_SIZE = (224, 224)  # Resizing frames\n",
    "NUM_FRAMES = 30  # Number of frames to sample per video\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8069c7-0a3f-4ac7-a32a-8517a24b80f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"Extract a fixed number of frames from a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if idx in frame_indices and ret:\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def load_data_from_folders(video_folder):\n",
    "    \"\"\"Load videos and their labels from separate folders.\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label, subfolder in enumerate(['real', 'fake']):  # 0 = real, 1 = fake\n",
    "        folder_path = os.path.join(video_folder, subfolder)\n",
    "        for video_name in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, video_name)\n",
    "            frames = extract_frames(video_path)\n",
    "            if len(frames) == NUM_FRAMES:\n",
    "                data.append(frames)\n",
    "                labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd40752d-c400-40fd-bfa3-fe504f406ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/editor/DFD/process\\\\real'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize pixel values to [0, 1]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_data_from_folders\u001b[1;34m(video_folder)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, subfolder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m'\u001b[39m]):  \u001b[38;5;66;03m# 0 = real, 1 = fake\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_folder, subfolder)\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m video_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     21\u001b[0m         video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, video_name)\n\u001b[0;32m     22\u001b[0m         frames \u001b[38;5;241m=\u001b[39m extract_frames(video_path)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:/editor/DFD/process\\\\real'"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "data, labels = load_data_from_folders(VIDEO_PATH)\n",
    "labels = np.array(labels, dtype=np.float32)\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f051861-02c9-46a4-b8c6-b69d2ef397de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_base = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "cnn_base.trainable = False  # Freeze the CNN\n",
    "\n",
    "def extract_cnn_features(frames):\n",
    "    \"\"\"Extract features for a sequence of frames.\"\"\"\n",
    "    features = np.array([cnn_base.predict(frame[None, ...])[0] for frame in frames])  # Use [0] to remove the extra axis\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebb8c3-c49d-46ac-8d37-19fc849c4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting CNN features...\")\n",
    "cnn_features = np.array([extract_cnn_features(video) for video in data])\n",
    "print(f\"Feature shape: {cnn_features.shape}\")  # Ensure this is (num_videos, NUM_FRAMES, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d29f0-0ae5-47a3-92a3-ce1ce4dcad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = cnn_features.shape[-1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(NUM_FRAMES, num_features)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc9e9d-4ed5-49ab-b6ff-1e813fb6a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    cnn_features, labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00cbe2-4c84-48b5-9527-c3e1879c7b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating model...\")\n",
    "loss, accuracy = model.evaluate(cnn_features, labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "end_time = time.time()\n",
    "print(f'Total time for training {(end_time-start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f883399-cde4-4278-8c8a-7c2470a0a338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and accuracy from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c3bc7-64c3-491e-88da-a000ec362b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deepfake_detection_model.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7977f-eb55-4063-9c3b-a2d48683636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deepfake_detection_model.keras\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7edefeb-2e16-4cfa-886f-dd62d23dbcbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=video_detector.pth. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(video_detector.pth, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(frames) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize pixel values to [0, 1]\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m test_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual test video path\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aditha Ayomal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=video_detector.pth. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(video_detector.pth, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "FRAME_SIZE = (224, 224)  # Resizing frames\n",
    "NUM_FRAMES = 30  # Number of frames to sample per video\n",
    "MODEL_PATH = \"video_detector.pth\"  # Path to the trained model\n",
    "\n",
    "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"Extract a fixed number of frames from a test video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if idx in frame_indices and ret:\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "test_video_path = \"test1.mp4\"  # Replace with the actual test video path\n",
    "print(f\"Testing video: {test_video_path}\")\n",
    "\n",
    "frames = extract_frames(test_video_path)\n",
    "\n",
    "if len(frames) == NUM_FRAMES:\n",
    "    # Extract features using the CNN base\n",
    "    from tensorflow.keras.applications import ResNet50\n",
    "    cnn_base = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    cnn_base.trainable = False  # Ensure the CNN is not trainable during testing\n",
    "\n",
    "    features = np.array([cnn_base.predict(frame[None, ...])[0] for frame in frames])\n",
    "    features = np.expand_dims(features, axis=0)  # Add batch dimension for model input\n",
    "\n",
    "    # Predict the class of the test video\n",
    "    prediction = model.predict(features)\n",
    "    print(prediction)\n",
    "    print(\"Prediction:\", \"Real\" if prediction[0] < 0.5 else \"Fake\")\n",
    "else:\n",
    "    print(f\"Insufficient frames extracted. Expected {NUM_FRAMES}, got {len(frames)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
