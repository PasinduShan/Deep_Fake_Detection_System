{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28741a72-d60f-4c99-bf91-026b68ec198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1db27b-e871-428d-abd8-556bfaacfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"D:/editor/DFD/process\"  # Parent folder containing \"real/\" and \"fake/\"\n",
    "FRAME_SIZE = (224, 224)  # Resizing frames\n",
    "NUM_FRAMES = 30  # Number of frames to sample per video\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8069c7-0a3f-4ac7-a32a-8517a24b80f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"Extract a fixed number of frames from a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if idx in frame_indices and ret:\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def load_data_from_folders(video_folder):\n",
    "    \"\"\"Load videos and their labels from separate folders.\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label, subfolder in enumerate(['real', 'fake']):  # 0 = real, 1 = fake\n",
    "        folder_path = os.path.join(video_folder, subfolder)\n",
    "        for video_name in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, video_name)\n",
    "            frames = extract_frames(video_path)\n",
    "            if len(frames) == NUM_FRAMES:\n",
    "                data.append(frames)\n",
    "                labels.append(label)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd40752d-c400-40fd-bfa3-fe504f406ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.4 GiB for an array with shape (3419, 30, 224, 224, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize pixel values to [0, 1]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mload_data_from_folders\u001b[1;34m(video_folder)\u001b[0m\n\u001b[0;32m     24\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(frames)\n\u001b[0;32m     25\u001b[0m             labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(labels)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.4 GiB for an array with shape (3419, 30, 224, 224, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "data, labels = load_data_from_folders(VIDEO_PATH)\n",
    "labels = np.array(labels, dtype=np.float32)\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f051861-02c9-46a4-b8c6-b69d2ef397de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_base = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "cnn_base.trainable = False  # Freeze the CNN\n",
    "\n",
    "def extract_cnn_features(frames):\n",
    "    \"\"\"Extract features for a sequence of frames.\"\"\"\n",
    "    features = np.array([cnn_base.predict(frame[None, ...])[0] for frame in frames])  # Use [0] to remove the extra axis\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebb8c3-c49d-46ac-8d37-19fc849c4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting CNN features...\")\n",
    "cnn_features = np.array([extract_cnn_features(video) for video in data])\n",
    "print(f\"Feature shape: {cnn_features.shape}\")  # Ensure this is (num_videos, NUM_FRAMES, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d29f0-0ae5-47a3-92a3-ce1ce4dcad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = cnn_features.shape[-1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(NUM_FRAMES, num_features)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc9e9d-4ed5-49ab-b6ff-1e813fb6a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    cnn_features, labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00cbe2-4c84-48b5-9527-c3e1879c7b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating model...\")\n",
    "loss, accuracy = model.evaluate(cnn_features, labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "end_time = time.time()\n",
    "print(f'Total time for training {(end_time-start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f883399-cde4-4278-8c8a-7c2470a0a338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and accuracy from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c3bc7-64c3-491e-88da-a000ec362b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deepfake_detection_model.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7977f-eb55-4063-9c3b-a2d48683636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deepfake_detection_model.keras\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edefeb-2e16-4cfa-886f-dd62d23dbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "FRAME_SIZE = (224, 224)  # Resizing frames\n",
    "NUM_FRAMES = 30  # Number of frames to sample per video\n",
    "MODEL_PATH = \"deepfake_detection_model.h5\"  # Path to the trained model\n",
    "\n",
    "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
    "    \"\"\"Extract a fixed number of frames from a test video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if idx in frame_indices and ret:\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "test_video_path = \"D:/editor/DFD/Fake/01_03__hugging_happy__ISF9SP4G.mp4\"  # Replace with the actual test video path\n",
    "print(f\"Testing video: {test_video_path}\")\n",
    "\n",
    "frames = extract_frames(test_video_path)\n",
    "\n",
    "if len(frames) == NUM_FRAMES:\n",
    "    # Extract features using the CNN base\n",
    "    from tensorflow.keras.applications import ResNet50\n",
    "    cnn_base = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    cnn_base.trainable = False  # Ensure the CNN is not trainable during testing\n",
    "\n",
    "    features = np.array([cnn_base.predict(frame[None, ...])[0] for frame in frames])\n",
    "    features = np.expand_dims(features, axis=0)  # Add batch dimension for model input\n",
    "\n",
    "    # Predict the class of the test video\n",
    "    prediction = model.predict(features)\n",
    "    print(prediction)\n",
    "    print(\"Prediction:\", \"Real\" if prediction[0] < 0.5 else \"Fake\")\n",
    "else:\n",
    "    print(f\"Insufficient frames extracted. Expected {NUM_FRAMES}, got {len(frames)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da45e4-4899-4d6b-b9e3-70af9729514f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
